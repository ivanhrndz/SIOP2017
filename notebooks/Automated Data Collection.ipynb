{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Data Collection With Python\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Steps of Automatically Collecting Data Online **\n",
    "\n",
    "+ Download the HTML Source of a page\n",
    "+ Extract the content from the HTML\n",
    "+ Save the Content\n",
    "+ Repeat the Process on A Different Page\n",
    "\n",
    "All of these steps can be automated, running indepdent of human interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import the needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning, make sure you have Python installed on your computer with the necessary libraries.\n",
    "\n",
    "If you do not have Python installed, a recommended distribution is **Anaconda**: [https://www.continuum.io/downloads](https://www.continuum.io/downloads)\n",
    "\n",
    "The 2.7 version of Python is recommended for people new to Python.\n",
    "\n",
    "We are first going to load two key libraries\n",
    "+ the urllib library\n",
    "+ the BeautifulSoup library\n",
    "\n",
    "The \"urllib\" library allows us to access a webpage with Python and download the HTML\n",
    "\n",
    "The \"BeautifulSoup\" library allows us to parse the HTML of a webpage and isolate content within HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the HTML Source of a page\n",
    "\n",
    "\n",
    "+ 1.1 Direct Python to Access the Webpage, and Save the Page's HTML to a variable\n",
    "\n",
    "+ 1.2 Pass the downloaded HTML to an HTML Parser in Python (BeautifulSoup)\n",
    "\n",
    "+ 1.3 Examine the downloaded content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Direct Python to Access the Webpage, and Save the Page's HTML to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of scraping information from the web is downloading the source code of a specific webpage.\n",
    "\n",
    "You should have an idea in mind of the page(s) that contain the information you need.\n",
    "\n",
    "When you know what page you need to extract content from, then you can direct Python to download it.\n",
    "\n",
    "The code below:\n",
    " + Create a variable, called \"url\", that has the address of the webpage we want to scrape\n",
    " + Download the html for the webpage and save it as a variable called \"sourcecode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://ivanhernandez.com/example.html\"\n",
    "sourcecode = urllib.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pass the downloaded HTML to an HTML Parser in Python (BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the webpage downloaded, we need to parse through the elements of the source code in an easy way.\n",
    "\n",
    "The BeautifulSoup library allows us to easily access the different elements of a webpage.\n",
    "\n",
    "The code below, passes the source code to the parser, and we can query the different elements of the page through the \"soup\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(sourcecode, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the contents of the source code using the \"prettify\" function, which displays the source code of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Example Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This Title is in an H1 tag\n",
      "  </h1>\n",
      "  <div class=\"box1\">\n",
      "   This text is inside a div tag, whose class is equal to box1\n",
      "  </div>\n",
      "  <br>\n",
      "   <div class=\"box2\">\n",
      "    This text is inside a div tag, whose class is equal to box2\n",
      "   </div>\n",
      "   <br>\n",
      "    <span class=\"box3\">\n",
      "     This text is inside a span tag, whose class is equal to box3\n",
      "    </span>\n",
      "    <p id=\"box4\">\n",
      "     This text is inside a p tag, whose id is equal to box4\n",
      "    </p>\n",
      "    <a href=\"http://google.com\">\n",
      "     This is a link to Google\n",
      "    </a>\n",
      "    <br>\n",
      "     <br>\n",
      "      Additional Content: This content is not inside any tag\n",
      "     </br>\n",
      "    </br>\n",
      "   </br>\n",
      "  </br>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Extracting Content from a Page\n",
    "\n",
    "Once you have the page source downloaded and parsed, you are ready to tell Python to extract the content you want.\n",
    "\n",
    "\n",
    "If we know the type of tag (div, p, a, etc.), and we know an identifying feature (class, id, name, etc.), we can grab the content in that specified tag.\n",
    "\n",
    "Depending on your project and goals, you will find yourself in one these possible situations:\n",
    "\n",
    "+ 2.1 You want a **Single Piece of Text** Data from a Page\n",
    " + a. You want content from a specific tag with a specific id/class/name\n",
    " + b. You want content from a specific tag, and the id/class/name does not matter\n",
    "\n",
    "\n",
    "+ 2.2 You want **Multiple Pieces of Text** Data from a Page\n",
    " + a. You want all the content from a specific tag that occurs many times\n",
    " + b. You want all the content that could come from many possible tags\n",
    " + c. You want all the content that could come from many possible id/class names\n",
    " + d. You want all the content that comes from a specific tag, and partially matches an id/class/name\n",
    "\n",
    "\n",
    "+ 2.3 You want **Information from Links**\n",
    " + a. You want to get the link text\n",
    " + b. You want to get the url of a link\n",
    "\n",
    "\n",
    "+ 2.4 You want to Extract **Non-tagged** Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Collecting a Single Piece of Text Data from a Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a When you know the tag name and identifying information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the content from the div tag with a class equal to \"box1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This text is inside a div tag, whose class is equal to box1'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"div\", {\"class\": \"box1\"})\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the content from the div tag with a class equal to \"box2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This text is inside a div tag, whose class is equal to box2'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"div\", {\"class\": \"box2\"})\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b When you only know/need the tag name\n",
    "\n",
    "Let's grab the content from the p tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This text is inside a p tag, whose id is equal to box4'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"p\")\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the content from the span tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This text is inside a span tag, whose class is equal to box3'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"span\")\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Collecting Multiple Pieces of Text Data from a Page\n",
    "\n",
    "If there are multiple tags you want to extract content from, you can use the \"findAll\" function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a When you know the tag name\n",
    "\n",
    "For example, there are two div tags in the entire HTML (the one with class=box1 and the one with class=box2). \n",
    "\n",
    "If we wanted to extract all of the content at once, then just ask BeautifulSoup to find all the div tags and save them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"box1\">This text is inside a div tag, whose class is equal to box1</div>,\n",
       " <div class=\"box2\">This text is inside a div tag, whose class is equal to box2</div>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = soup.findAll(\"div\")\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The findAll function saves the results in a list (indicated by the surrounding square-brackets)\n",
    "\n",
    "We can iteratively access the specific content in the list using a \"for loop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\n",
      "This text is inside a div tag, whose class is equal to box2\n"
     ]
    }
   ],
   "source": [
    "for content in contents:\n",
    "    print content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.b When there are many different tag names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to extract content that could be in either a div or a span or a p tag, then, we can place them in a list (within square brackets), and run the findAll function using that list of tags.\n",
    "\n",
    "Below, we specify that we want returned any content within a div, span, or p tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\n",
      "This text is inside a div tag, whose class is equal to box2\n",
      "This text is inside a span tag, whose class is equal to box3\n",
      "This text is inside a p tag, whose id is equal to box4\n"
     ]
    }
   ],
   "source": [
    "contents = soup.findAll([\"div\",\"span\",\"p\"])\n",
    "for content in contents:\n",
    "    print content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.c When there are many different id/class names\n",
    "\n",
    "If we know precisely the names of the classes/ids that could match, we can specify the tag name, as well all the id and class names we want to match in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\n",
      "This text is inside a div tag, whose class is equal to box2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "contents = soup.findAll(\"div\", {\"class\" : [\"box1\",\"box2\"]})\n",
    "\n",
    "for content in contents:\n",
    "    print content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.d When you only know the tag and part of the class name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may be a situation where we know what we want to extract (e.g., we want something that is in between div tags , that has a class with the word \"box\" in it).\n",
    "\n",
    "Using the regular expression library (whose library is called \"re\"), we can have partial matches of tags or classes/ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\n",
      "This text is inside a div tag, whose class is equal to box2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "contents = soup.findAll(\"div\", {\"class\" : re.compile('box.*')})\n",
    "\n",
    "for content in contents:\n",
    "    print content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract content from a link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have link that, if clicked, direct the user to a different page. We can extract various content from a link including\n",
    "+ The text the user sees for the link\n",
    "+ The address the link directs the user to go, when clicked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.a Extract the text from the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This is a link to Google'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"a\")\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.b Extract the url from the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'http://google.com'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find(\"a\")\n",
    "content[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extract Everything Else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have content that is not contained within tags, or has irregular structure. We can still extract this information if we know the characters that come immediately before and immediately after the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Determine the HTML that goes immediately before the content (precontent)\n",
    "  + In this example, \"Additional Content:\" came before the text we want to extract\n",
    "  + We will split the HTML on where it says \"Additional Content:\" and keep the text after it\n",
    "  \n",
    "  \n",
    "+ Determine the HTML that goes immediately after the content (postcontent)\n",
    "  + In this example, \"<\" came immediately after the text we want to extract\n",
    "  + We will split the HTML that we kept on where it says \"<\" and keep the text before it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' This content is not inside any tag\\r\\n\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text.split(\"Additional Content:\")[1].split(\"<\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Saving the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have extracted the content, we need to preserve the information for subsequent analysis.\n",
    "\n",
    "You can save the content you extracted in a\n",
    "+ list (within Python's memory)\n",
    "+ text file (on your hard drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Saving the content to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"box1\"})\n",
    "data.append(content.text)\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"box2\"})\n",
    "data.append(content.text)\n",
    "\n",
    "content = soup.find(\"span\", {\"class\": \"box3\"})\n",
    "data.append(content.text)\n",
    "\n",
    "content = soup.find(\"p\", {\"id\": \"box4\"})\n",
    "data.append(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\n",
      "This text is inside a div tag, whose class is equal to box2\n",
      "This text is inside a span tag, whose class is equal to box3\n",
      "This text is inside a p tag, whose id is equal to box4\n"
     ]
    }
   ],
   "source": [
    "for item in data:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Saving the contents to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textfile = open(\"data.txt\",\"ab\")\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"box1\"})\n",
    "textfile.write(content.text)\n",
    "textfile.write(\"\\r\\n\")\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"box2\"})\n",
    "textfile.write(content.text)\n",
    "textfile.write(\"\\r\\n\")\n",
    "\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the contents of a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is inside a div tag, whose class is equal to box1\r\n",
      "This text is inside a div tag, whose class is equal to box2\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textfile = open(\"data.txt\",\"rb\")\n",
    "print textfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loop through many pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you only want a single piece of information from each page\n",
    "\n",
    "\n",
    "Below, we know that we want to access the health section, the techonology section, and the science section of the New York Times. If we know ahead of time the ways we want to modify the url to access the pages,\n",
    "\n",
    "+ Start with a list of what we want to add to the url\n",
    "+ Make an empty list to hold all of the data you collect\n",
    "+ Make a url by combining a baseline url with the part we want to add\n",
    "+ Go through each page\n",
    "+ On each page, extract the data\n",
    "+ Add the data to the list that holds all of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages = [\"health\",\"technology\",\"science\"]\n",
    "\n",
    "titles = []\n",
    "for topic in pages: \n",
    "    url = \"https://www.nytimes.com/section/\" + topic \n",
    "    sourcecode = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(sourcecode, 'html.parser')\n",
    "    title = soup.find('title')\n",
    "    titles.append(title.text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Health - The New York Times', u'Technology - The New York Times', u'Science - The New York Times']\n"
     ]
    }
   ],
   "source": [
    "print titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you want multiple pieces of information from each page\n",
    "\n",
    "We may want to access multiple pieces of content, from many different webpages. This situation requires two for-loops. The first loop goes through each webpage extension (\"health\", \"technology\", \"science), to access the specific topic's page. Then, when we are on the page, we with find all of the relevant content, and make a for-loop that goes through each result and appends it to our list of results.\n",
    "\n",
    "+ Start with a list of webpage urls\n",
    "+ Make an empty list to hold all of the data you collect\n",
    "+ Make a For-Loop to go through each page\n",
    "+ On each page, find all instances of the tag\n",
    "+ Make a For-Loop to go through each instance found, and extract the data\n",
    "+ Add the data to the list that holds all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = [\"health\",\"technology\",\"science\"]\n",
    "\n",
    "headlinelist = []\n",
    "for topic in pages:\n",
    "    url = \"https://www.nytimes.com/section/\" + topic \n",
    "    sourcecode = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(sourcecode, 'html.parser')\n",
    "    contents = soup.findAll([\"h2\",\"h3\"],{\"class\":\"headline\"})\n",
    "    \n",
    "    for content in contents:\n",
    "        \n",
    "        headline = content.text #Get the text\n",
    "        headlinelist.append(headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print headlinelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Everything\n",
    "\n",
    "There are many possibilities for what you can do. You can combine the different functions together to collect the links from one page and then use those links to collect data from the subsequent page.\n",
    "\n",
    "The code below, retrieves all of the links to job specific pages, and then goes to each of the links and retreives the job name and median wage.\n",
    "\n",
    "Finally, it writes both pieces of information to a textfile for every job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=\"https://www.onetonline.org/find/family?f=27&g=Go\"\n",
    "sourcecode = urllib.urlopen(url)\n",
    "soup = BeautifulSoup(sourcecode, 'html.parser')\n",
    "joblinks = soup.findAll(\"a\", {\"href\": re.compile(\"https://www.onetonline.org/link/summary.*\")})\n",
    "\n",
    "for link in joblinks:\n",
    "    url = \"https://www.onetonline.org/link/details/\"+jobid.text\n",
    "    sourcecode = urllib.urlopen(url)\n",
    "    soup = BeautifulSoup(sourcecode, 'html.parser')\n",
    "    income = soup.text.split(\"Median wages (2016)\")[1].split(\" \")[0].strip()\n",
    "    job = soup.find(\"title\").text\n",
    "    textfile = open(\"data.txt\",\"a\")\n",
    "    textfile.write(job + \",\" + income + \"\\r\\n\")\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Automated Data Collection\n",
    "\n",
    "+ Decide on what content you want to scrape\n",
    "+ Get a list of the page urls you want to scrape\n",
    "+ Make a For-Loop that goes through every single page\n",
    "+ Scrape the content from each page\n",
    "+ Save the content to a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional notes\n",
    "\n",
    "+ Use the time.sleep(10) function to pause your script's execution for 10 seconds to not overload the server you are trying to collect\n",
    "+ Be respectful of a site's Terms of Service Policies\n",
    "+ Use the \"mechanize\" library to fill in forms for websites that want you to sign-in or enter information to access the content\n",
    "+ Try different things: Often there is not just one solution for collecting the data - many ways to parse the HTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
